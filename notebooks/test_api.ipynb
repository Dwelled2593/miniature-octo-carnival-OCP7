{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de l'API Credit Scoring\n",
    "\n",
    "Ce notebook permet de tester l'API de prédiction de score crédit de manière interactive.\n",
    "\n",
    "## Objectifs\n",
    "- Tester tous les endpoints de l'API\n",
    "- Valider les prédictions\n",
    "- Analyser les SHAP values\n",
    "- Benchmarker les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "API_URL = \"http://localhost:8080\"  # Modifier selon votre déploiement\n",
    "# API_URL = \"https://your-cloud-run-url.run.app\"  # Pour Cloud Run\n",
    "\n",
    "print(f\"API URL: {API_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_health():\n",
    "    \"\"\"Vérifier l'état de l'API\"\"\"\n",
    "    response = requests.get(f\"{API_URL}/health\")\n",
    "    return response.json()\n",
    "\n",
    "health = check_health()\n",
    "print(json.dumps(health, indent=2))\n",
    "\n",
    "assert health[\"status\"] == \"healthy\", \"API is not healthy!\"\n",
    "assert health[\"model_loaded\"] == True, \"Model is not loaded!\"\n",
    "print(\"\\n✅ API is healthy and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données de test\n",
    "test_client = {\n",
    "    \"features\": {\n",
    "        \"EXT_SOURCE_2\": 0.5,\n",
    "        \"EXT_SOURCE_3\": 0.6,\n",
    "        \"DAYS_BIRTH\": -15000,\n",
    "        \"AMT_CREDIT\": 500000.0,\n",
    "        \"AMT_ANNUITY\": 25000.0,\n",
    "        \"AMT_GOODS_PRICE\": 450000.0,\n",
    "        \"DAYS_EMPLOYED\": -3000,\n",
    "        \"DAYS_ID_PUBLISH\": -2000,\n",
    "        \"REGION_POPULATION_RELATIVE\": 0.02,\n",
    "        \"DAYS_LAST_PHONE_CHANGE\": -1000\n",
    "    },\n",
    "    \"client_id\": \"TEST_001\"\n",
    "}\n",
    "\n",
    "# Faire la prédiction\n",
    "response = requests.post(f\"{API_URL}/predict\", json=test_client)\n",
    "prediction = response.json()\n",
    "\n",
    "print(\"Prediction Result:\")\n",
    "print(json.dumps(prediction, indent=2))\n",
    "\n",
    "# Visualisation\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Client ID: {prediction['client_id']}\")\n",
    "print(f\"Decision: {prediction['decision']}\")\n",
    "print(f\"Probability of Default: {prediction['probability_default']:.2%}\")\n",
    "print(f\"Probability of No Default: {prediction['probability_no_default']:.2%}\")\n",
    "print(f\"Threshold Used: {prediction['threshold_used']:.2%}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les SHAP values\n",
    "response = requests.post(f\"{API_URL}/feature-importance\", json=test_client)\n",
    "shap_data = response.json()\n",
    "\n",
    "print(\"SHAP Analysis:\")\n",
    "print(f\"Base Value: {shap_data['base_value']:.4f}\")\n",
    "print(f\"Prediction Value: {shap_data['prediction_value']:.4f}\")\n",
    "\n",
    "print(\"\\nTop Positive Features (contribute to approval):\")\n",
    "for feat in shap_data['top_positive_features'][:5]:\n",
    "    print(f\"  {feat['feature']}: {feat['value']:.4f}\")\n",
    "\n",
    "print(\"\\nTop Negative Features (contribute to rejection):\")\n",
    "for feat in shap_data['top_negative_features'][:5]:\n",
    "    print(f\"  {feat['feature']}: {feat['value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des SHAP values\n",
    "top_features = (shap_data['top_positive_features'][:10] + \n",
    "                shap_data['top_negative_features'][:10])\n",
    "\n",
    "# Trier par valeur absolue\n",
    "top_features.sort(key=lambda x: abs(x['value']), reverse=True)\n",
    "\n",
    "features = [f['feature'] for f in top_features]\n",
    "values = [f['value'] for f in top_features]\n",
    "colors = ['green' if v > 0 else 'red' for v in values]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(features, values, color=colors)\n",
    "plt.xlabel('SHAP Value')\n",
    "plt.title('Feature Importance (SHAP Values)')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Predictions Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer plusieurs clients de test\n",
    "batch_request = {\n",
    "    \"clients\": [\n",
    "        {\n",
    "            \"features\": {**test_client[\"features\"], \"EXT_SOURCE_2\": 0.3},\n",
    "            \"client_id\": \"BATCH_001\"\n",
    "        },\n",
    "        {\n",
    "            \"features\": {**test_client[\"features\"], \"EXT_SOURCE_2\": 0.5},\n",
    "            \"client_id\": \"BATCH_002\"\n",
    "        },\n",
    "        {\n",
    "            \"features\": {**test_client[\"features\"], \"EXT_SOURCE_2\": 0.7},\n",
    "            \"client_id\": \"BATCH_003\"\n",
    "        },\n",
    "        {\n",
    "            \"features\": {**test_client[\"features\"], \"EXT_SOURCE_3\": 0.8},\n",
    "            \"client_id\": \"BATCH_004\"\n",
    "        },\n",
    "        {\n",
    "            \"features\": {**test_client[\"features\"], \"EXT_SOURCE_3\": 0.4},\n",
    "            \"client_id\": \"BATCH_005\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Faire les prédictions batch\n",
    "response = requests.post(f\"{API_URL}/predict/batch\", json=batch_request)\n",
    "batch_results = response.json()\n",
    "\n",
    "print(f\"Total Clients: {batch_results['total_clients']}\")\n",
    "print(f\"Approved: {batch_results['approved_count']}\")\n",
    "print(f\"Rejected: {batch_results['rejected_count']}\")\n",
    "\n",
    "# Créer un DataFrame pour analyse\n",
    "df_results = pd.DataFrame(batch_results['predictions'])\n",
    "print(\"\\nResults:\")\n",
    "print(df_results[['client_id', 'decision', 'probability_default', 'probability_no_default']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats batch\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des décisions\n",
    "decision_counts = df_results['decision'].value_counts()\n",
    "axes[0].pie(decision_counts.values, labels=decision_counts.index, autopct='%1.1f%%',\n",
    "            colors=['green', 'red'])\n",
    "axes[0].set_title('Distribution des Décisions')\n",
    "\n",
    "# Distribution des probabilités\n",
    "axes[1].hist(df_results['probability_default'], bins=10, edgecolor='black')\n",
    "axes[1].axvline(x=df_results['threshold_used'].iloc[0], color='red', \n",
    "                linestyle='--', label='Threshold')\n",
    "axes[1].set_xlabel('Probability of Default')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution des Probabilités de Défaut')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de performance\n",
    "n_requests = 100\n",
    "latencies = []\n",
    "\n",
    "print(f\"Running {n_requests} prediction requests...\")\n",
    "\n",
    "for i in range(n_requests):\n",
    "    start = time.time()\n",
    "    response = requests.post(f\"{API_URL}/predict\", json=test_client)\n",
    "    latency = (time.time() - start) * 1000  # Convert to ms\n",
    "    latencies.append(latency)\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Completed {i + 1}/{n_requests}\")\n",
    "\n",
    "# Statistiques\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Performance Statistics:\")\n",
    "print(f\"  Mean Latency: {sum(latencies)/len(latencies):.2f} ms\")\n",
    "print(f\"  Median Latency: {sorted(latencies)[len(latencies)//2]:.2f} ms\")\n",
    "print(f\"  Min Latency: {min(latencies):.2f} ms\")\n",
    "print(f\"  Max Latency: {max(latencies):.2f} ms\")\n",
    "print(f\"  P95 Latency: {sorted(latencies)[int(len(latencies)*0.95)]:.2f} ms\")\n",
    "print(f\"  P99 Latency: {sorted(latencies)[int(len(latencies)*0.99)]:.2f} ms\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des latences\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(latencies)\n",
    "plt.xlabel('Request Number')\n",
    "plt.ylabel('Latency (ms)')\n",
    "plt.title('Latency Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(latencies, bins=30, edgecolor='black')\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Latency Distribution')\n",
    "plt.axvline(x=sum(latencies)/len(latencies), color='red', \n",
    "            linestyle='--', label='Mean')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec données invalides\n",
    "print(\"Testing error handling...\\n\")\n",
    "\n",
    "# Test 1: Empty features\n",
    "try:\n",
    "    response = requests.post(f\"{API_URL}/predict\", json={\"features\": {}})\n",
    "    print(f\"Empty features: Status {response.status_code}\")\n",
    "    print(f\"Response: {response.json()}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Empty features error: {e}\\n\")\n",
    "\n",
    "# Test 2: Missing features field\n",
    "try:\n",
    "    response = requests.post(f\"{API_URL}/predict\", json={\"client_id\": \"TEST\"})\n",
    "    print(f\"Missing features: Status {response.status_code}\")\n",
    "    print(f\"Response: {response.json()}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Missing features error: {e}\\n\")\n",
    "\n",
    "# Test 3: Invalid endpoint\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/invalid-endpoint\")\n",
    "    print(f\"Invalid endpoint: Status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Invalid endpoint error: {e}\\n\")\n",
    "\n",
    "print(\"✅ Error handling tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"API TEST SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAPI URL: {API_URL}\")\n",
    "print(f\"API Status: {health['status']}\")\n",
    "print(f\"Model Loaded: {health['model_loaded']}\")\n",
    "print(f\"API Version: {health['version']}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Average Latency: {sum(latencies)/len(latencies):.2f} ms\")\n",
    "print(f\"  P95 Latency: {sorted(latencies)[int(len(latencies)*0.95)]:.2f} ms\")\n",
    "print(f\"\\nBatch Test Results:\")\n",
    "print(f\"  Total Clients: {batch_results['total_clients']}\")\n",
    "print(f\"  Approved: {batch_results['approved_count']}\")\n",
    "print(f\"  Rejected: {batch_results['rejected_count']}\")\n",
    "print(f\"  Approval Rate: {batch_results['approved_count']/batch_results['total_clients']*100:.1f}%\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ All tests completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}